services:
  # =====================================================
  # PostgreSQL MDM Hub - Base de données principale
  # =====================================================
  postgres-mdm-hub:
    image: postgres:15-alpine
    container_name: postgres-mdm-hub
    environment:
      POSTGRES_DB: mdm_clinique
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: root
      PGDATA: /var/lib/postgresql/data/pgdata
    ports:
      - "5432:5432"
    volumes:
      - postgres_mdm_data:/var/lib/postgresql/data
      - ./sql/00-enable-pg-stat-statements.sql:/docker-entrypoint-initdb.d/00-enable-pg-stat-statements.sql
      - ./sql/01-create-tables.sql:/docker-entrypoint-initdb.d/01-create-tables.sql
    command: >
      postgres
      -c shared_preload_libraries=pg_stat_statements
      -c pg_stat_statements.max=10000
      -c pg_stat_statements.track=all
    networks:
      - mdm-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d mdm_clinique"]
      interval: 10s
      timeout: 5s
      retries: 5

  # =====================================================
  # OpenMetadata - Base de données
  # NOTE: Ces services sont sous le profil "openmetadata"
  # Pour les démarrer: docker-compose --profile openmetadata up -d
  # Port externe 5433 pour éviter les conflits avec PostgreSQL local
  # =====================================================
  openmetadata-db:
    image: docker.getcollate.io/openmetadata/postgresql:1.10.5
    container_name: openmetadata-db
    restart: always
    command: "--work_mem=10MB"
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
    ports:
      - "5433:5432"  # Port externe 5433 pour éviter les conflits
    volumes:
      - openmetadata_db_data:/var/lib/postgresql/data
    networks:
      - mdm-network
    healthcheck:
      test: ["CMD-SHELL", "psql -U postgres -tAc 'select 1' -d openmetadata_db"]
      interval: 15s
      timeout: 10s
      retries: 10
    profiles:
      - openmetadata
      - openmetadata-init  # Inclus aussi dans openmetadata-init pour la migration

  # =====================================================
  # Elasticsearch pour OpenMetadata
  # NOTE: Sous profil "openmetadata" pour éviter les conflits
  # =====================================================
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.4
    container_name: openmetadata-elasticsearch
    environment:
      - discovery.type=single-node
      - ES_JAVA_OPTS=-Xms1024m -Xmx1024m
      - xpack.security.enabled=false
    ports:
      - "9200:9200"
      - "9300:9300"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    networks:
      - mdm-network
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:9200/_cluster/health?pretty | grep status | grep -qE 'green|yellow' || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 10
    profiles:
      - openmetadata
      - openmetadata-init  # Inclus aussi dans openmetadata-init pour la migration

  # =====================================================
  # OpenMetadata - Migration (one-time)
  # Exécutez ce service une seule fois pour initialiser la base de données
  # =====================================================
  openmetadata-migrate:
    image: docker.getcollate.io/openmetadata/server:1.10.5
    container_name: openmetadata-migrate
    command: "./bootstrap/openmetadata-ops.sh migrate"
    environment:
      OPENMETADATA_CLUSTER_NAME: openmetadata
      SERVER_PORT: 8585
      SERVER_ADMIN_PORT: 8586
      LOG_LEVEL: INFO
      DB_DRIVER_CLASS: org.postgresql.Driver
      DB_SCHEME: postgresql
      DB_USER: openmetadata_user
      DB_USER_PASSWORD: openmetadata_password
      DB_HOST: openmetadata-db
      DB_PORT: 5432
      OM_DATABASE: openmetadata_db
      ELASTICSEARCH_HOST: elasticsearch
      ELASTICSEARCH_PORT: 9200
      ELASTICSEARCH_SCHEME: http
      SEARCH_TYPE: elasticsearch
      EVENT_MONITOR: prometheus
      SECRET_MANAGER: db
      OPENMETADATA_HEAP_OPTS: -Xmx1G -Xms1G
      AUTHENTICATION_PROVIDER: basic
    depends_on:
      elasticsearch:
        condition: service_healthy
      openmetadata-db:
        condition: service_healthy
    networks:
      - mdm-network
    profiles:
      - openmetadata-init

  # =====================================================
  # OpenMetadata Server
  # =====================================================
  openmetadata-server:
    image: docker.getcollate.io/openmetadata/server:1.10.5
    container_name: openmetadata-server
    restart: always
    ports:
      - "8585:8585"
      - "8586:8586"
    environment:
      OPENMETADATA_CLUSTER_NAME: openmetadata
      SERVER_PORT: 8585
      SERVER_ADMIN_PORT: 8586
      LOG_LEVEL: INFO
      AUTHENTICATION_PROVIDER: basic
      DB_DRIVER_CLASS: org.postgresql.Driver
      DB_SCHEME: postgresql
      DB_USER: openmetadata_user
      DB_USER_PASSWORD: openmetadata_password
      DB_HOST: openmetadata-db
      DB_PORT: 5432
      OM_DATABASE: openmetadata_db
      ELASTICSEARCH_HOST: elasticsearch
      ELASTICSEARCH_PORT: 9200
      ELASTICSEARCH_SCHEME: http
      SEARCH_TYPE: elasticsearch
      EVENT_MONITOR: prometheus
      SECRET_MANAGER: db
      OPENMETADATA_HEAP_OPTS: -Xmx1G -Xms1G
      # Configuration pour se connecter à l'Airflow d'ingestion
      # Utiliser le nom du conteneur Docker au lieu de localhost
      PIPELINE_SERVICE_CLIENT_ENDPOINT: http://openmetadata-ingestion:8080
      AIRFLOW_USERNAME: admin
      AIRFLOW_PASSWORD: admin
      # VARIABLE CLÉ : URL du serveur OpenMetadata utilisée dans les workflows générés
      # Cette URL doit être accessible depuis le conteneur openmetadata-ingestion
      SERVER_HOST_API_URL: http://openmetadata-server:8585/api
    depends_on:
      elasticsearch:
        condition: service_healthy
      openmetadata-db:
        condition: service_healthy
    networks:
      - mdm-network
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8586/healthcheck"]
      interval: 30s
      timeout: 10s
      retries: 5
    profiles:
      - openmetadata

  # =====================================================
  # OpenMetadata Ingestion (Airflow pour les DAGs d'ingestion)
  # NOTE: Ce service crée son propre Airflow interne pour exécuter les DAGs d'ingestion
  # Port externe 8080 pour OpenMetadata, port 8081 pour l'Airflow du projet MDM
  # =====================================================
  openmetadata-ingestion:
    image: docker.getcollate.io/openmetadata/ingestion:1.10.5
    container_name: openmetadata-ingestion
    depends_on:
      openmetadata-server:
        condition: service_started
      elasticsearch:
        condition: service_started
      openmetadata-db:
        condition: service_healthy
    environment:
      AIRFLOW__API__AUTH_BACKENDS: "airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session"
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__OPENMETADATA_AIRFLOW_APIS__DAG_GENERATED_CONFIGS: "/opt/airflow/dag_generated_configs"
      # Configuration de la base de données Airflow pour l'ingestion
      # Utilise la même base openmetadata-db mais avec un schéma différent
      DB_HOST: openmetadata-db
      DB_PORT: 5432
      AIRFLOW_DB: airflow_db
      DB_USER: airflow_user
      DB_SCHEME: postgresql+psycopg2
      DB_PASSWORD: airflow_pass
      # Configuration de la base de données Airflow pour l'ingestion
      # L'URL du serveur OpenMetadata est déterminée par SERVER_HOST_API_URL dans openmetadata-server
      # Pas besoin de variables OPENMETADATA_SERVER_URL ici
    entrypoint: /bin/bash
    command:
      - "/opt/airflow/ingestion_dependency.sh"
    expose:
      - 8080
    ports:
      - "8080:8080"  # Port externe 8080 pour OpenMetadata Ingestion
    networks:
      - mdm-network
    volumes:
      - openmetadata_ingestion_dag_configs:/opt/airflow/dag_generated_configs
      - openmetadata_ingestion_dags:/opt/airflow/dags
      - openmetadata_ingestion_tmp:/tmp
      # Ajouter les DAGs du projet MDM pour qu'ils soient aussi dans le port 8080
      # Airflow lit récursivement tous les fichiers .py dans /opt/airflow/dags
      - ./airflow/dags:/opt/airflow/dags/project_mdm:ro
      - ./airflow/config:/opt/airflow/config/project_mdm:ro
      - ./talend_jobs:/opt/airflow/talend_jobs:ro
      - ./data:/opt/airflow/data:ro
    profiles:
      - openmetadata

  # =====================================================
  # Airflow - Base de données metadata
  # =====================================================
  airflow-db:
    image: postgres:15-alpine
    container_name: airflow-db
    environment:
      POSTGRES_DB: airflow
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
    ports:
      - "5434:5432"
    volumes:
      - airflow_db_data:/var/lib/postgresql/data
    networks:
      - mdm-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow -d airflow"]
      interval: 10s
      timeout: 5s
      retries: 5

  # =====================================================
  # Airflow - Redis (pour CeleryExecutor)
  # =====================================================
  airflow-redis:
    image: redis:7-alpine
    container_name: airflow-redis
    ports:
      - "6379:6379"
    networks:
      - mdm-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # =====================================================
  # Airflow - Webserver
  # NOTE: Port externe 8081 pour éviter les conflits avec d'autres services
  # =====================================================
  airflow-webserver:
    image: apache/airflow:2.8.0
    container_name: airflow-webserver
    depends_on:
      airflow-db:
        condition: service_healthy
      airflow-redis:
        condition: service_healthy
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-db/airflow
      - AIRFLOW__CORE__FERNET_KEY= <AIRFLOW__CORE__FERNET_KEY>
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.basic_auth
      - AIRFLOW__WEBSERVER__SECRET_KEY=mdm_airflow_secret_key_2024_secure_random_string_12345
      - AIRFLOW__CORE__SECURITY=use_default
      - _AIRFLOW_DB_UPGRADE=true
      - _AIRFLOW_WWW_USER_CREATE=true
      - _AIRFLOW_WWW_USER_USERNAME=admin
      - _AIRFLOW_WWW_USER_PASSWORD=admin
      - _AIRFLOW_WWW_USER_EMAIL=admin@example.com
      - _AIRFLOW_WWW_USER_FIRSTNAME=Admin
      - _AIRFLOW_WWW_USER_LASTNAME=User
      - _AIRFLOW_WWW_USER_ROLE=Admin
      - _PIP_ADDITIONAL_REQUIREMENTS=
    user: "${AIRFLOW_UID:-50000}:0"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/config:/opt/airflow/config
      - ./talend_jobs:/opt/airflow/talend_jobs
      - ./data:/opt/airflow/data
    ports:
      - "8081:8080"  # Port externe 8081 pour éviter les conflits
    command: webserver
    networks:
      - mdm-network
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 60s

  # =====================================================
  # Airflow - Scheduler
  # =====================================================
  airflow-scheduler:
    image: apache/airflow:2.8.0
    container_name: airflow-scheduler
    depends_on:
      airflow-db:
        condition: service_healthy
      airflow-redis:
        condition: service_healthy
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-db/airflow
      - AIRFLOW__CORE__FERNET_KEY=7tvFSa7K0A2qIZRi61TqD6szQxgBrsUSmCPUolQy5pw=
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.basic_auth
      - AIRFLOW__WEBSERVER__SECRET_KEY=mdm_airflow_secret_key_2024_secure_random_string_12345
      - AIRFLOW__CORE__SECURITY=use_default
      - _PIP_ADDITIONAL_REQUIREMENTS=
    user: "${AIRFLOW_UID:-50000}:0"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/config:/opt/airflow/config
      - ./talend_jobs:/opt/airflow/talend_jobs
      - ./data:/opt/airflow/data
    command: scheduler
    networks:
      - mdm-network
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type SchedulerJob --hostname "$${HOSTNAME}" || exit 1']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  # =====================================================
  # Airflow - Init DB (one-time initialization)
  # Utilisez ce service avec --profile init pour initialiser la base
  # =====================================================
  airflow-init:
    image: apache/airflow:2.8.0
    container_name: airflow-init
    depends_on:
      airflow-db:
        condition: service_healthy
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-db/airflow
      - AIRFLOW__CORE__FERNET_KEY=7tvFSa7K0A2qIZRi61TqD6szQxgBrsUSmCPUolQy5pw=
      - AIRFLOW__CORE__SECURITY=use_default
      - _AIRFLOW_DB_UPGRADE=true
      - _AIRFLOW_WWW_USER_CREATE=true
      - _AIRFLOW_WWW_USER_USERNAME=admin
      - _AIRFLOW_WWW_USER_PASSWORD=admin
      - _AIRFLOW_WWW_USER_EMAIL=admin@example.com
      - _AIRFLOW_WWW_USER_FIRSTNAME=Admin
      - _AIRFLOW_WWW_USER_LASTNAME=User
      - _AIRFLOW_WWW_USER_ROLE=Admin
    user: "${AIRFLOW_UID:-50000}:0"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/config:/opt/airflow/config
      - ./talend_jobs:/opt/airflow/talend_jobs
      - ./data:/opt/airflow/data
    entrypoint: /bin/bash
    command:
      - -c
      - |
        echo "Initializing Airflow DB..."
        airflow db migrate || airflow db upgrade || true
        echo "Airflow initialization completed"
    networks:
      - mdm-network
    profiles:
      - init

  # =====================================================
  # Zookeeper (pour Kafka)
  # =====================================================
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
      - zookeeper_log:/var/lib/zookeeper/log
    networks:
      - mdm-network
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost 2181 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  # =====================================================
  # Kafka
  # =====================================================
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,PLAINTEXT_INTERNAL://kafka:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT_INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    ports:
      - "9092:9092"
    volumes:
      - kafka_data:/var/lib/kafka/data
    networks:
      - mdm-network
    healthcheck:
      test: ["CMD-SHELL", "kafka-broker-api-versions --bootstrap-server localhost:9092 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

# =====================================================
# Volumes
# =====================================================
volumes:
  postgres_mdm_data:
    driver: local
  openmetadata_db_data:
    driver: local
  elasticsearch_data:
    driver: local
  airflow_db_data:
    driver: local
  zookeeper_data:
    driver: local
  zookeeper_log:
    driver: local
  kafka_data:
    driver: local
  openmetadata_ingestion_dag_configs:
    driver: local
  openmetadata_ingestion_dags:
    driver: local
  openmetadata_ingestion_tmp:
    driver: local

# =====================================================
# Networks
# =====================================================
networks:
  mdm-network:
    driver: bridge

